root{
    adapt_docx_desc{"makes some changes to .xml files from .docx components\n"
                    "- moves word boundaries to 'legal' positions\n"
                    "Gets input file name from stdin (or from -f), writes to stdout"}
    usage{"USAGE"}
    options{"Options"}
    arguments{"ARGUMENTS"}
    name_desc{"writes \"< file name=\"filename\">\" to output"}
    file_desc{"gets file name as parameter"}
    pretty_desc{"outputs xml with a pretty format"}
    paragraphs_found{"{num} paragraphs found"}

    compile_caps_desc{"compile capitalization restoration rules"}
    help_desc{"print this message and exit"}

    deshtml_desc{"html format processor."}

    extract_caps_desc{"Transfer capitalization information to word-bound blanks"}
    surface_desc{"keep surface forms"}
    null_flush_desc{"flush output on the null character"}

    interchunck_desc{"process stream with interchunker"}
    trace_desc{"trace mode (show rule numbers and patterns matched)"}
    dictionary_case_desc{"ignore capitalization manipulation instructions"}
    interchunk_epilog{"FILES:\n"
                       "  t2x        t2x rules file\n"
                       "  preproc    result of preprocess trules file\n"
                       "  input      input file, standard input by default\n"
                       "  output     output file, standard output by default"}
    postchunk_epilog{"FILES:\n"
                       "  t3x        t3x rules file\n"
                       "  preproc    result of preprocess trules file\n"
                       "  input      input file, standard input by default\n"
                       "  output     output file, standard output by default"}

    perceptron_trace_desc{"Run with one of:\n"
                          "{program_name} model <binary model file>\n"
                          "Output features and weights from a model file.\n"
                          "{program_name} mtx <mtx file>\n"
                          "Output macros and features from an mtx file.\n"
                          "{program_name} path <mtx file> <untagged> <tagged>\n"
                          "Trace a particular path through giving which features fire "
                          "and the resulting score. Useful for interactively "
                          "designing feature sets.\n"}
    postchunck_desc{"process stream with postchunker"}

    no_surface_forms_desc{"assume no surface forms"}
    compounds_desc{"treat ~ as compound separator"}

    restore_caps_desc{"compile capitalization restoration rules"}
    keep_desc{"retain all wblanks"}

    tagger_apply_new_rules_desc{"Forbid and enforce rules are applied to the given HMM parameters"}
    hmm_filein_desc{"To specify the file with the HMM parameter to process"}
    hmm_fileout_desc{"To specify the file to which the HMM will be written"}
    hmm_tsxfile_desc{"File containing the rules to apply"}
    tagger_apply_new_rules_note{"NOTE: Parameters are read from and written to the files provided"}

    reading_from_file{"Reading from file \"{file_name}\" ... "}
    writing_to_file{"Writing to file \"{file_name}\" ... "}
    done{"done."}
    readed_words{"{number_of_words} were readed."}

    probfile_desc{"Specify a tagger parameter file"}
    clength_desc{"Specify the length of the corpus to process"}
    locale{"LOCALE"}
    command_line{"Command line"}
    tagger_note{"Try \"apertium-tagger --help\" for more information."}
    percent_desc{"number 0 < n <= 1 to set margin of confidence of TU's \n"
                 "                (0.85 by default) in length terms."}
    edit_desc{"number 0 < n <= 1 to set margin of confidence of TU's \n"
              "                (0.30 by default) in edit distance terms"}
    low_limit_desc{"ignore percent if the segment is less than lowlimit \n"
                   "                (15 by default)"}
    max_edit_desc{"characters to be taken into account when aligning \n"
                  "                sentences (50 by default)"}
    diagonal_desc{"diagonal width for using edit distance, 10 by default"}
    window_desc{"window size of the edit distance with sentences \n"
                "                (100 sentences by default)"}
    step_desc{"step for moving the window during the alingment \n"
              "                (75 sentences by default)"}
    tmxbuild_note{"Other parameters:\n"
                  "  code1, code2 codes of the languages (i.e. ISO-631 ones)\n"
                  "  doc1, doc2    unformatted docs to build the TMX file\n"
                  "  output_file   if not specified, the result will be printed to stdout"}
    trace_att_desc{"trace, for apertium-transfer-tools (also sets -t)"}
    from_bilingual_desc{"input from lexical transfer"}
    no_bilingual_desc{"don't use bilingual dictionary"}
    extended_desc{"extended mode with user dictionary"}
    case_sensitive_desc{"case-sensitiveness while accessing bilingual dictionary"}
    trules_desc{"transfer rules file"}
    preproc_desc{"result of preprocess trules file"}
    biltrans_desc{"bilingual letter transducer file"}
    input_desc{"input file, standard input by default"}
    output_desc{"output file, standard output by default"}

    apertium_desc{"USAGE: {basename} [-d datadir] [-f format] [-u] <direction> [in [out]]\n"
                  " -d datadir       directory of linguistic data\n"
                  " -f format        one of: txt (default), html, rtf, odt, odp, docx, wxml, xlsx, pptx,\n"
                  "                  xpresstag, html-noent, html-alt, latex, latex-raw, line\n"
                  " -a               display ambiguity\n"
                  " -u               don't display marks '*' for unknown words\n"
                  " -n               don't insert period before possible sentence-ends\n"
                  " -m memory.tmx    use a translation memory to recycle translations\n"
                  " -o direction     translation direction using the translation memory,\n"
                  "                  by default 'direction' is used instead\n"
                  " -l               lists the available translation directions and exits\n"
                  " -V               print Apertium version\n"
                  " -z               force null-flush mode on all parts of the pipe\n"
                  " direction        typically, LANG1-LANG2, but see modes.xml in language data\n"
                  " in               input file (stdin by default)\n"
                  " out              output file (stdout by default)"}

    gen_modes_desc{"Generate mode command files from XML"}
    full_desc{"expect absolute installation path"}
    local_desc{"output to current directory rather than directory of modes.xml"}
    verbose_desc{"print more detailed messages"}
    num_of_states_and_ambiguity_classes{"{states} states and {classes} ambiguity classes."}

    interchunk_rule_line{"apertium-interchunk: Rule {value} line {line}"}
    postchunk_rule_line{"apertium-postchunk: Rule {value} line {line}"}

    tagger_cc_note{"Mandatory arguments to long options are mandatory for short options too."}
    tagger_debug_desc{"with -g, print error messages about the input"}
    tagger_first_desc{"with -g, reorder each lexical unit's analyses so that the chosen one is first"}
    tagger_mark_desc{"with -g, mark disambiguated lexical units"}
    tagger_show_superficial_desc{"with -g, output each lexical unit's surface form"}
    tagger_null_flush_desc{"with -g, flush output on the null character"}
    tagger_unigram_desc{"use unigram algorithm MODEL from <https://coltekin.net/cagri/papers/trmorph-tools.pdf>"}
    tagger_sliding_window_desc{"use the Light Sliding Window algorithm"}
    tagger_perceptron_desc{"use the averaged perceptron algorithm"}
    tagger_skip_on_error_desc{"with -xs, ignore certain types of errors with the training corpus"}
    tagger_desc{"disambiguate the input"}
    tagger_retrain_desc{"with -u: exit;\notherwise: retrain the tagger with ITERATIONS unsupervised iterations"}
    tagger_supervised_desc{"with -u: train the tagger with a hand-tagged corpus;\nwith -w: exit;\notherwise: initialise the tagger with a hand-tagged corpus and retrain it with ITERATIONS tagger_unsupervised iterations"}
    tagger_train_desc{"with -u: exit;\notherwise: train the tagger with ITERATIONS unsupervised iterations"}
    apertium_tagger_desc{"  -d, --debug             with -g, print error messages about the input\
  -f, --first             with -g, reorder each lexical unit's analyses so that\
                            the chosen one is first\
  -m, --mark              with -g, mark disambiguated lexical units\
  -p, --show-superficial  with -g, output each lexical unit's surface form\
  -z, --null-flush        with -g, flush the output after getting each null\
                            character\
\
  -u, --unigram=MODEL  use unigram algorithm MODEL from\
                         <https://coltekin.net/cagri/papers/trmorph-tools.pdf>\
\
  -w, --sliding-window  use the Light Sliding Window algorithm\
  -x, --perceptron      use the averaged perceptron algorithm\
  -e, --skip-on-error   with -xs, ignore certain types of errors with the\
                          training corpus\
\
  -g, --tagger  disambiguate the input\
\
  -r, --retrain=ITERATIONS     with -u: exit;\
                                 otherwise: retrain the tagger with ITERATIONS\
                                 unsupervised iterations\
  -s, --supervised=ITERATIONS  with -u: train the tagger with a hand-tagged\
                                 corpus;\
                                 with -w: exit;\
                                 otherwise: initialise the tagger with a\
                                 hand-tagged corpus and retrain it with\
                                 ITERATIONS unsupervised iterations\
  -t, --train=ITERATIONS       with -u: exit;\
                                 otherwise: train the tagger with ITERATIONS\
                                 unsupervised iterations\
"}
    tmx_aligner_tool_desc{"Usage (either):\n\
    alignerTool [ common_arguments ] [ -hand=hand_align_file ] dictionary_file source_text target_text\n\
\n\
or:\n\
    alignerTool [ common_arguments ] -batch dictionary_file batch_file\n\
\n\
where\n\
common_arguments ::= [ -text ] [ -bisent ] [ -utf ] [ -cautious ] [ -realign [ -autodict=filename ] ]\n\
    [ -thresh=n ] [ -ppthresh=n ] [ -headerthresh=n ] [ -topothresh=n ]\n\
\n\
Arguments:\n\
\n\
-text\n\
	The output should be in text format, rather than the default (numeric) ladder format.\n\
\n\
-bisent\n\
	Only bisentences (one-to-one alignment segments) are printed. In non-text mode, their\n\
	starting rung is printed.\n\
\n\
-cautious\n\
	In -bisent mode, only bisentences for which both the preceding and the following\n\
	segments are one-to-one are printed. In the default non-bisent mode, only rungs\n\
	for which both the preceding and the following segments are one-to-one are printed.\n\
\n\
-hand=file\n\
	When this argument is given, the precision and recall of the alignment is calculated\n\
	based on the manually built ladder file. Information like the following is written\n\
	on the standard error: \n\
	53 misaligned out of 6446 correct items, 6035 bets.\n\
	Precision: 0.991218, Recall: 0.928017\n\
	\n\
        Note that by default, 'item' means rung. The switch -bisent also changes the semantics\n\
	of the scoring from rung-based to bisentence-based and in this case 'item' means bisentences.\n\
	See File formats about the format of this input align file.\n\
\n\
-autodict=filename\n\
	The dictionary built during realign is saved to this file. By default, it is not saved.\n\
\n\
-utf\n\
	The system uses the character counts of the sentences as information for the\n\
	pairing of sentences. By default, it assumes one-byte character encoding such\n\
	as ISO Latin-1 when calculating these counts. If our text is in UTF-8 format,\n\
	byte counts and character counts are different, and we must use the -utf switch\n\
	to force the system to properly calculate character counts.\n\
	Note: UTF-16 input is not supported.\n\
\n\
Postfiltering options:\n\
There are various postprocessors which remove implausible rungs based on various heuristics.\n\
\n\
-thresh=n\n\
	Don't print out segments with score lower than n/100.\n\
\n\
-ppthresh=n\n\
	Filter rungs with less than n/100 average score in their vicinity.\n\
\n\
-headerthresh=n\n\
	Filter all rungs at the start and end of texts until finding a reliably\n\
	plausible region.\n\
\n\
-topothresh=n\n\
	Filter rungs with less than n percent of one-to-one segments in their vicinity.\n\
\n\
"}
    wblank_attach_desc{
      "Distributes word-bound blanks across all tokens they encompass, turning [[A]]^...$^...$[[/]] into [[A]]^...$[[A]]^...$\n"
      "Also merges word-bound blanks, turning [[A]][[B]]^...$^...$[[/]][[/]] into [[A; B]]^...$\n"
      "Word-bound blanks will be deduplicated, but order will be preserved amongst unique elements.\n"}
    wblank_detach_desc{
      "Closes all word-bound blanks, turning [[...]]^...$ into [[...]]^...$[[/]]\n"
      "This tool does not merge across whitespace or do any other heuristics wrt. which word-bound blanks should have their spans combined.\n"}

    deformat_desc{"USAGE: {first_line}\n"
                  "  -a: apertium standard mode\n"
                  "  -A: apertium optimized mode (default mode)\n"
                  "  -m: matxin standard mode\n"}
    gen_desc{"USAGE: {first_line}\n"}
    trans_desc{"USAGE: <datadir> <translation> [format [infile [outfile]]]\n"
                  " datadir          Directory of linguistic data\n"
                  " translation      LANG1-LANG2\n"
                  " format           one of: txt (default), txtu, html, htmlu, rtf, rtfu\n"
                  " infile           input file (stdin by default)\n"
                  " outfile          output file (stdout by default)"}
    some_failed_assertion{"some failed assertion: {what}"}
    unformat_desc{"USAGE: {basename} [-f format] [in [out]]\n"
                  " -f format        one of: txt (default), html, rtf, odt, docx, wxml, xlsx, pptx\n"
                  " in               input file (stdin by default)\n"
                  " out              output file (stdout by default)"}
    APR80000{"ERROR APR80000: Unable to access file \"{file_name}\"."}
    APR80010{"ERROR APR80010: Unescaped \"^\": {buf}^"}
    APR80020{"ERROR APR80020: Stray \"$\""}
    APR80120{"ERROR APR80120: In {file_name} on line {line_number}: Unexpected tag <{tag}>."}
    APR80130{"ERROR APR80130: Can't convert const Analysis & comprising empty Morpheme std::vector to {letter}."}
    APR80140{"ERROR APR80140: Can't convert const Analysis & comprising Morpheme comprising empty Tag std::vector to {letter}."}
    APR80150{"ERROR APR80150: Can't convert Analysis comprising empty Morpheme std::vector to UString."}
    APR80160{"ERROR APR80160: Unterminated lexical unit."}
    APR80170{"ERROR APR80170: Reading regexp."}
    APR80180{"ERROR APR80180: Unable to compile regular expression \"{exp}\".\n"
             "Error code: {error}"}
    APR80190{"ERROR APR80190: Cannot write empty regexp."}
    APR80200{"ERROR APR80200: Unable to apply regexp.\n"
             "Error code: {error}"}
    APR80210{"ERROR APR80210: Unable to extract substring from regexp match.\n"
             "Error code: {error}"}
    APR80220{"ERROR APR80220: You did not provide an input file (.prob). Use --filein to do that."}
    APR80230{"ERROR APR80230: You did not provide an output file (.prob). Use --fileout to do that."}
    APR80240{"ERROR APR80240: You did not provide a tagger definition file (.tsx). Use --filetsx to do that."}
    APR80250{"ERROR APR80250: Ambiguity class number out of range: {number}\n"
             "Word: {word}\n"
             "Ambiguity class: {class}"}
    APR80260{"ERROR APR80260: Corpus length provided with --clength must be a positive integer."}
    APR80270{"ERROR APR80270: You have provided neither a tagger specification file (.tsx) nor a tagger probability file (.prob). Use --tsxfile or --probfile to provide one of them."}
    APR80280{"ERROR APR80280: You provided a tagger specification file and a tagger probability file. Only one of them can be provided, not both."}
    APR80290{"ERROR APR80290: In {file_name} on line {line_number}: Attribute lemma conflicts with attribute trglem."}
    APR80300{"ERROR APR80300: In {file_name} on line {line_number}: Attribute surface conflicts with attribute trgsurf."}
    APR80310{"ERROR APR80310: In {file_name} on line {line_number}: Unknown select value \"{select}\"."}
    APR80320{"ERROR APR80320: In {file_name} on line {line_number}: Number of repetitions cannot be negative."}
    APR80330{"ERROR APR80330: In {file_name} on line {line_number}: Lower bound on number of repetitions cannot be larger than upper bound."}
    APR60340{"WARNING APR60340: There is not coarse tag for the fine tag \"{substr}\" of \"{str}\"\n"
             "                  This is because of an incomplete tagset definition or a dictionary error"}
    APR60350{"WARNING APR60350: kIGNORE was returned while reading a word.\n"
             "                  Word being read: {word}\n"
             "                  Debug: {str}"}
    APR60360{"WARNING APR60360: Debug mode name {debug_name} generated multiple times, disregarding result from {mode_name} step {step_num}."}
    APR80370{"ERROR APR80370: {program}:Installation prefix is the same directory as modes.xml; give a different INSTALLDIR."}
    APR80380{"ERROR APR80380: Tagged text (.tagged) and analyzed text (.untagged) streams are not aligned.\n"
             "Take a look at tagged text (.tagged).\n"
             "Perhaps this is caused by a multiword unit that is not a multiword unit in one of the two files.\n"
             "{word_tagged} -- {word_untagged}"}
    APR80390{"ERROR APR80390: word_untagged==NULL"}
    APR80400{"ERROR APR80400: Error in tagged text. An ambiguous word was found: {word}"}
    APR60410{"WARNING APR60410: The last tag is not the end-of-sentence-tag but rather {tag}. Line: {line}. Pending: {pending}."}
    APR60420{"WARNING APR60420: gamma[{index}]=0"}
    APR60430{"WARNING APR60430: Problem with word \"{word}\" {tags}"}
    APR60440{"WARNING APR60440: The text to disambiguate has finished, but there are ambiguous words that has not been disambiguated.\n"
             "This message should never appears. If you are reading this ..... these are very bad news."}
    APR80450{"ERROR APR80450: Can't convert const Morpheme & comprising empty Tag std::vector to {letter}."}
    APR80460{"ERROR APR80460: Can't convert const Analysis & comprising Morpheme comprising empty Tag std::vector to {letter}."}
    APR80470{"ERROR APR80470: In {file_name} on line {line_number}: index >= limit"}
    APR80480{"ERROR APR80480: In {file_name} on line {line_number}: index < 0"}
    APR80490{"ERROR APR80490: In {file_name} on line {line_number}: Null access at word[index]"}
    APR80500{"ERROR APR80500: Unexpected expression: \"{expression}\""}
    APR80510{"ERROR APR80510: Unknown input token."}
    APR60530{"WARNING APR60530: apertium-interchunck: on line {line_number}: {tag} sometimes discards its value."}
    APR80540{"ERROR APR80540: Can't convert const Analysis & comprising Morpheme comprising empty Lemma UString to Lemma."}
    APR80550{"ERROR APR80550: Can't convert const Morpheme & comprising empty Lemma UString to Lemma."}
    APR80560{"ERROR APR80560: Can't convert Morpheme comprising empty Tag std::vector to UString."}
    APR80570{"ERROR APR80570: Can't convert Morpheme comprising empty TheLemma UString to UString."}
    APR80580{"ERROR APR80580: Unterminted lexical unit."}
    APR80590{"ERROR APR80590: empty lemma."}
    APR80600{"ERROR APR80600: invalid tag <>."}
    APR80610{"ERROR APR80610: morpheme has no tags."}
    APR80620{"ERROR APR80620: trailing backslash."}
    APR80630{"ERROR APR80630: unexpected < after lemma queue."}
    APR80640{"ERROR APR80640: On line {line}, column {column}: Expected set-member."}
    APR80650{"ERROR APR80650: On line {line}, column {column}: Expected an integer expression."}
    APR80660{"ERROR APR80660: On line {line}, column {column}: Expected a string list expression."}
    APR80670{"ERROR APR80670: On line {line}, column {column}: No such argument {var}."}
    APR80680{"ERROR APR80680: On line {line}, column {column}: Variable {var} has the wrong type."}
    APR80690{"ERROR APR80690: On line {line}, column {column}: Variable {var} has not been set."}
    APR80700{"ERROR APR80700: On line {line}, column {column}: No such macro {var}."}
    APR80710{"ERROR APR80710: On line {line}, column {column}: Macro {var} returns the wrong type."}
    APR80720{"ERROR APR80720: On line {line}, column {column}: Expected a string expression."}
    APR80730{"ERROR APR80730: On line {line}, column {column}: Expected an address expression."}
    APR80740{"ERROR APR80740: On line {line}, column {column}: Expected a wordoid array expression."}
    APR80750{"ERROR APR80750: On line {line}, column {column}: Expected a wordoid expression."}
    APR80760{"ERROR APR80760: On line {line}, column {column}: Set required."}
    APR80770{"ERROR APR80770: On line {line}, column {column}: String required."}
    APR80780{"ERROR APR80780: On line {line}, column {column}: No {what} named {name}."}
    APR80790{"ERROR APR80790: On line {line}, column {column}: {what} required."}
    APR80800{"ERROR APR80800: On line {line}, column {column}: Opcodes can have at most one operand."}
    APR80810{"ERROR APR80810: On line {line}, column {column}: Expected a string, bool or int expression."}
    APR80820{"ERROR APR80820: On line {line}, column {column}: \"as\" attribute required for for-each."}
    APR80830{"ERROR APR80830: On line {line}, column {column}: Expected a string array or wordoid array expression."}
    APR80840{"ERROR APR80840: On line {line}, column {column}: Expected a void expression."}
    APR80850{"ERROR APR80850: On line {line}, column {column}: \"as\" attribute required for def-macro."}
    APR80860{"ERROR APR80860: On line {line}, column {column}: Expected a non-void expression."}
    APR80870{"ERROR APR80870: On line {line}, column {column}: expected <metatag> tag."}
    APR80880{"ERROR APR80880: Unimplemented opcode: {opstr} at {feature} #{feat_idx} address #{bytecode_idx}"}
    APR80890{"ERROR APR80890: Tagged analysis unavailable in untagged/ambigous input.\n"
             "Available:\n"
             "{available}"
             "Required: {required}\n"
             "Rerun with --skip-on-error to skip this sentence.\n"}
    APR80900{"ERROR APR80900: Skipped {skipped} sentences due to token misalignment and {avail_skipped} sentences due to tagged token being unavailable in untagged file out of {total} total sentences."}
    APR80910{"ERROR APR80910: In {file_name} on line {line_number}: index > limit"}
    APR60920{"WARNING APR60920: apertium-postchunk: on line {line_number}: {tag} sometimes discards its value."}
    APR80930{"ERROR APR80930: Postchunk::processCallMacro() assumes npar > 0, but got npar <= 0"}
    APR60940{"WARNING APR60940: Not calling macro \"{macro}\" from line {line} (empty word?)"}
    APR80950{"ERROR APR80950: Unexpected EOF"}
    APR80960{"ERROR APR80960: Wordbound blank isn't immediately followed by the Lexical Unit."}
    APR60970{"WARNING APR60970: Streams diverged at line {tagged_line}\n"
             "Untagged token: {untagged_token}\n"
             "Tagged token: {tagged_token}\n"
             "Rerun with --skip-on-error to skip this sentence."}
    APR80980{"ERROR APR80980: One stream has ended prematurely.\n"
             "Please check if they are aligned.\n"}
    APR80990{"ERROR APR80990: Expected one of this file arguments {expected}, got {actual}"}
    APR81000{"ERROR APR81000: Failed operation in {metavar} file \"{filename}\""}
    APR81010{"ERROR APR81010: lexical unit has no analyses"}
    APR81020{"ERROR APR81020: unexpected /, surface form is empty"}
    APR81030{"ERROR APR81030: unterminated lexical unit"}
    APR81040{"ERROR APR81040: A new ambiguity class was found. I cannot continue.\n"
             "Word \"{word}\" not found in the dictionary.\n"
             "New ambiguity class: {class}\n"
             "Line Number: {line}\n"
             "Take a look at the dictionary, then retrain."}
    APR61050{"WARNING APR61050: A new ambiguity class was found. \n"
             "Retraining the tagger is necessary so as to take it into account.\n"
             "Word \"{word}\".\n"
             "New ambiguity class: {class}"}
    APR81060{"ERROR APR81060: reading map"}
    APR81070{"ERROR APR81070: invalid argument \"{optarg}\" for \"--unigram\"\n"
             "Valid arguments are:\n"
             "  - \"1\"\n"
             "  - \"2\"\n"
             "  - \"3\""}
    APR81080{"ERROR APR81080: invalid option -- \"{opt}\""}
    APR81090{"ERROR APR81090: unexpected \"{opt1}\" following \"{opt2}\""}
    APR81100{"ERROR APR81100: invalid argument \"{arg}\" for \"{opt}\""}
    APR81110{"ERROR APR81110: can't convert {metavar} \"{val}\" to unsigned long"}
    APR81120{"ERROR APR81120: can't convert {metavar} of size 1 to unsigned long"}
    APR81130{"ERROR APR81130: can't convert {metavar} \"{val}\" to unsigned long, not in unsigned long range"}
    APR81140{"ERROR APR81140: can't deserialise SERIALISED_TAGGER file \"{file}\" Reason: {what}"}
    APR81150{"ERROR APR81150: no space in line"}
    APR81160{"ERROR APR81160: too much data in line"}
    APR81170{"ERROR APR81170: Batch mode requires exactly two file arguments."}
    APR81180{"ERROR APR81180: -batch and -{arg} are incompatible switches."}
    APR81190{"ERROR APR81190: -{arg} switch requires a filename value."}
    APR81200{"ERROR APR81200: Nonbatch mode requires exactly three file arguments."}
    APR81210{"ERROR APR81210: Batch file has incorrect format."}
    APR81220{"ERROR APR81220: data error."}
    APR81230{"ERROR APR81230: unimplemented."}
    APR81240{"ERROR APR81240: argument error."}
    APR81250{"ERROR APR81250: hopelessly bad trail."}
    APR81260{"ERROR APR81260: unable to parse argument \"{arg}\"."}
    APR81270{"ERROR APR81270: Empty argument"}
    APR81280{"ERROR APR81280: Argument -{arg}: integer expected."}
    APR81290{"ERROR APR81290: Argument -{arg}: value is not allowed."}
    APR81300{"ERROR APR81300: No value is allowed for argument -{arg}."}
    APR81310{"ERROR APR81310: Invalid argument: "}
    APR61320{"WARNING APR61320: conflict in tree"}
    APR81330{"ERROR APR81330: Incorrect bicorpus file: {records} records in line {line}"}
    APR81340{"ERROR APR81340: evalString() was called on a NULL element"}
    APR81350{"ERROR APR81350: You must specify either \"name\" or \"'namefrom\" for the \"chunk\" element"}
    APR61360{"WARNING APR61360: apertium-transfer: on line {line_number}: {tag} sometimes discards its value."}
    APR81370{"ERROR APR81370: Transfer::processLet() bad access on pos >= lword"}
    APR81380{"ERROR APR81380: Transfer::processLet() null access on word[pos]"}
    APR81390{"ERROR APR81390: On line {line}: processCallMacro() number of arguments >= npar"}
    APR81400{"ERROR APR81400: On line {line}, column {column}: <clip> missing attribute part."}
    APR81410{"ERROR APR81410: On line {line}, column {column}: Undefined attr-item {part}."}
    APR81420{"ERROR APR81420: On line {line}, column {column}: Undefined cat-item {attrib}."}
    APR61430{"WARNING APR61430: On line {line}: assignment to 'sl' side has no effect."}
    APR81440{"ERROR APR81440: On line {line}, column {column}: Macro \"{macro}\" defined at least twice"}
    APR81450{"ERROR APR81450: On line {line}, column {column}: \"{tag}\" already defined."}
    APR81460{"ERROR APR81460: On line {line}, column {column}: Unexpected \"{tag}\" tag."}
    APR81470{"ERROR APR81470: On line {line}, column {column}: <label-item> tag expected."}
    APR81480{"ERROR APR81480: can't serialise without first selecting a model."}
    APR81490{"ERROR APR81490: can't read tagger without first selecting a model."}
    APR81500{"ERROR APR81500: can't score analysis without first selecting a model."}
    APR81510{"ERROR APR81510: can't train model without first selecting a model."}
    APR81520{"ERROR APR81520: can't multiplyModel() without first selecting a model."}
    APR81530{"ERROR APR81530: can't train LexicalUnit comprising empty Analysis std::vector."}
    APR61540{"WARNING APR61540: tf-apertium-spread: Null-flush found, but had open word-bound blanks on line {line}:"}
    APR61550{"WARNING APR61550: tf-apertium-spread: Too many [[/]] on line  {line}:"}
    APR61560{"WARNING APR61560: tf-apertium-spread: End of input reached, but had an open blank."}
    APR81570{"ERROR APR81570: On line {line}, column {column}: unexpected EOF."}
    APR81580{"ERROR APR81580: Install an UTF-8 locale in your system."}
    APR81590{"ERROR APR81590: Input seems to be non-UTF-8, please convert to UTF-8 (e.g. with \"iconv -f {encoding} -t utf-8\")."}
    APR81600{"ERROR APR81600: APERTIUM_TRANSFUSE={APERTIUM_TRANSFUSE} but couldn't find Transfuse's tf-extract in PATH."}
    APR81610{"ERROR APR81610: Install \"{command}\" command in your system."}
    APR81620{"ERROR APR81620: {opt} requires an argument."}
    APR81630{"ERROR APR81630: Cannot compile TM {file}\n"
             "   hint: use -o parameter"}
    APR81640{"ERROR APR81640: Directory \"{directory}\" does not exist."}
    APR81650{"ERROR APR81650: Mode {mode} does not exist."}
    APR81660{"ERROR APR81660: Install a ISO-8859-1 compatible locale in your system."}
    APR61670{"WARNING APR61670: {arg}: Unknown format {format}, treating as \"txt\"."}
    APR81680{"ERROR APR81680: some unknown failed assertion..."}
    APR81690{"ERROR APR81690: Align failed for {file}"}
    APR81700{"ERROR APR81700: On line {line}, column {column}: Expected a boolean expression."}
    APR61710{"WARNING APR61710: tf-apertium-spread: Null-flush found, but had an open blank on line {line}:"}
}
